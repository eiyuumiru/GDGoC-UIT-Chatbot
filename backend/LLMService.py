from __future__ import annotations
from typing import Callable, List, Dict, Any, Optional
import os
from langchain_groq import ChatGroq
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.constants import END
from langgraph.graph import MessagesState, StateGraph
from langgraph.checkpoint.memory import MemorySaver
from langchain_core.runnables import RunnableConfig
from langchain_core.messages import SystemMessage, HumanMessage
from .FullChain import retrieve, _ensure_loaded


graph_builder = StateGraph(MessagesState)

SYSTEM_INSTRUCTIONS = (
    "B·∫°n l√† tr·ª£ l√Ω tr·∫£ l·ªùi c√¢u h·ªèi cho ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o UIT (Kh√≥a 2025). "
    "Ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n NG·ªÆ C·∫¢NH v√† th√¥ng tin ƒë∆∞·ª£c cung c·∫•p; n·∫øu thi·∫øu h√£y n√≥i r√µ "
    "'kh√¥ng c√≥ trong d·ªØ li·ªáu'. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c, ti·∫øng Vi·ªát. "
    "Khi c√≥ s·ªë li·ªáu ho·∫∑c quy ƒë·ªãnh, n√™u r√µ. "
    "Cu·ªëi c√πng ph·∫£i c√≥ m·ª•c 'Ngu·ªìn:' li·ªát k√™ c√°c ngu·ªìn ƒë√£ d√πng theo th·ª© t·ª± xu·∫•t hi·ªán."
)


def get_groq_llm(
    groq_api_key: Optional[str] = None,
    model: str = "llama-3.3-70b-versatile",
    temperature: float = 0.2,
    max_tokens: Optional[int] = None,
) -> ChatGroq:
    key = groq_api_key or os.getenv("GROQ_API_KEY")
    if not key:
        raise ValueError("Thi·∫øu GROQ_API_KEY")
    return ChatGroq(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
    )

def _format_context(chunks: List[Dict[str, Any]], max_chars: int = 8000) -> str:
    buf, total = [], 0
    for i, r in enumerate(chunks, 1):
        src = r.get("source") or ""
        content = (r.get("content") or "").strip()
        piece = f"<CTX id=\"{i}\" source=\"{src}\">\n{content}\n</CTX>"
        if total + len(piece) > max_chars:
            break
        buf.append(piece)
        total += len(piece)
    return "\n".join(buf)

def _few_shot_examples() -> str:
    ex_ctx = "<CTX id=\"1\" source=\"backend/dataset/majors/cs.md\">M√¥n CS311 h·ªçc k·ª≥ 6; t·ªïng s·ªë t√≠n ch·ªâ b·∫Øt bu·ªôc l√† 3.</CTX>\n<CTX id=\"2\" source=\"backend/dataset/policies/graduation.md\">ƒêi·ªÅu ki·ªán t·ªët nghi·ªáp: t√≠ch l≈©y ƒë·ªß t√≠n ch·ªâ, ho√†n th√†nh chu·∫©n ƒë·∫ßu ra.</CTX>"
    ex_user = "CS311 h·ªçc ·ªü k·ª≥ m·∫•y v√† c√≥ bao nhi√™u t√≠n ch·ªâ?"
    ex_out = "CS311 h·ªçc ·ªü k·ª≥ 6, 3 t√≠n ch·ªâ.\n\nNgu·ªìn: [1]"
    ex2_ctx = "<CTX id=\"1\" source=\"backend/dataset/majors/is.md\">Ng√†nh H·ªá th·ªëng th√¥ng tin: t·ªëi thi·ªÉu 125 t√≠n ch·ªâ.</CTX>"
    ex2_user = "S·ªë t√≠n ch·ªâ t·ªëi thi·ªÉu ng√†nh H·ªá th·ªëng th√¥ng tin?"
    ex2_out = "T·ªëi thi·ªÉu 125 t√≠n ch·ªâ.\n\nNgu·ªìn: [1]"
    return f"<EXAMPLES>\n<EXAMPLE>\n<CONTEXTS>\n{ex_ctx}\n</CONTEXTS>\n<QUESTION>{ex_user}</QUESTION>\n<IDEAL_ANSWER>{ex_out}</IDEAL_ANSWER>\n</EXAMPLE>\n<EXAMPLE>\n<CONTEXTS>\n{ex2_ctx}\n</CONTEXTS>\n<QUESTION>{ex2_user}</QUESTION>\n<IDEAL_ANSWER>{ex2_out}</IDEAL_ANSWER>\n</EXAMPLE>\n</EXAMPLES>"

def build_prompt(state: MessagesState):
    rules = (
        "Quy t·∫Øc:\n"
        "1) Ch·ªâ d√πng th√¥ng tin trong CONTEXTS.\n"
        "2) N·∫øu thi·∫øu d·ªØ li·ªáu c·∫ßn thi·∫øt, tr·∫£ l·ªùi: 'kh√¥ng c√≥ trong d·ªØ li·ªáu'.\n"
        "3) Tr√≠ch ngu·ªìn b·∫±ng ch·ªâ s·ªë [i] theo CTX id ƒë√£ cho.\n"
        "4) ∆Øu ti√™n li·ªát k√™ g·ªçn, r√µ.\n"
        "5) ƒê·∫ßu ra g·ªìm hai ph·∫ßn:\n"
        "   - Tr·∫£ l·ªùi: ph·∫ßn n·ªôi dung ch√≠nh\n"
        "   - Ngu·ªìn: danh s√°ch ch·ªâ s·ªë [i] theo th·ª© t·ª± s·ª≠ d·ª•ng"
    )

    # L·∫•y messages t·ª´ tool (ƒë√£ th·ª±c thi)
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    # Chuy·ªÉn ToolMessage th√†nh dict ƒë·ªÉ format
    docs_for_ctx = []
    for m in tool_messages:
        if hasattr(m, "artifact") and isinstance(m.artifact, list):
            for c in m.artifact:
                if isinstance(c, dict):
                    docs_for_ctx.append(c)

    ctx_block = _format_context(docs_for_ctx)
    fewshot = _few_shot_examples()

    conservation_messages = [
        message
        for message in state["messages"]
        if message.type in ("human", "system")
        or (message.type == "ai" and not message.tool_calls)
    ]

    system_message_content = (
        SYSTEM_INSTRUCTIONS + "\n\n" + rules + "\n\n" + ctx_block + "\n\n" + fewshot
    )
    prompt = [SystemMessage(content=system_message_content)] + conservation_messages
    return prompt

def create_rag_chain(
    groq_api_key: Optional[str],
    *,
    model: str = "llama-3.3-70b-versatile",
    temperature: float = 0.2,
    k: int = 6,
    max_ctx_chars: int = 8000,
) -> Callable[[str], Dict[str, Any]]:
    _ensure_loaded()
    llm = get_groq_llm(groq_api_key, model=model, temperature=temperature, max_tokens=7000)

    def query_or_response(state: MessagesState):
        """Generate tool call for retrieval or respond."""
        SYSTEM_PROMPT = SystemMessage(content = (
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω th√¥ng minh. "
            "Ch·ªâ s·ª≠ d·ª•ng c√¥ng c·ª• truy v·∫•n (retrieval tool) n·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ "
            "ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o, kh√≥a h·ªçc, ho·∫∑c th√¥ng tin li√™n quan ƒë·∫øn UIT. "
            "N·∫øu c√¢u h·ªèi mang t√≠nh x√£ giao (v√≠ d·ª•: ch√†o h·ªèi) ho·∫∑c kh√¥ng li√™n quan "
            "ƒë·∫øn UIT, h√£y tr·∫£ l·ªùi tr·ª±c ti·∫øp m√† kh√¥ng g·ªçi c√¥ng c·ª•."
        ))
        llm_with_tools = llm.bind_tools([retrieve])
        response = llm_with_tools.invoke([SYSTEM_PROMPT] + state['messages'])
        return {"messages": [response]}

    def generate_with_context(state: MessagesState):
        """Generate answer."""
        messages = build_prompt(state)
        out = llm.invoke(messages)
        return {"messages": [out]}

    # ToolNode ch·ªâ nh·∫≠n tool th·∫≠t s·ª±
    tools = ToolNode([retrieve])

    # Build graph
    graph_builder.add_node(query_or_response)
    graph_builder.add_node(tools)
    graph_builder.add_node(generate_with_context)

    graph_builder.set_entry_point("query_or_response")
    graph_builder.add_conditional_edges(
        "query_or_response",
        tools_condition,
        {END: END, "tools": "tools"},
    )
    graph_builder.add_edge("tools", "generate_with_context")
    graph_builder.add_edge("generate_with_context", END)
    graph = graph_builder.compile()

    def qa(question: str, topk: Optional[int] = None) -> Dict[str, Any]:
        result = graph.invoke({"messages": [HumanMessage(content=question)]})
        print(f"\n‚ùì C√¢u h·ªèi: {question}")
        for m in result.get("messages", []):
            if hasattr(m, "type") and m.type == "ai":
                if getattr(m, "tool_calls", None):
                    print("ü§ñ Model ch·ªçn ‚Üí TOOL CALL")
                else:
                    print("ü§ñ Model ch·ªçn ‚Üí TR·∫¢ L·ªúI TR·ª∞C TI·∫æP")
            elif hasattr(m, "type") and m.type == "tool":
                print("üõ†Ô∏è Tool output:", getattr(m, "content", None))
        return result

    return qa
