from __future__ import annotations
from typing import Callable, List, Dict, Any, Optional
import os

from langchain_groq import ChatGroq
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.constants import END, START
from langgraph.graph import MessagesState, StateGraph
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.prompts import ChatPromptTemplate

from .FullChain import retrieve, _ensure_loaded


def get_groq_llm(
    groq_api_key: Optional[str] = None,
    model: str = "llama-3.3-70b-versatile",
    temperature: float = 0.2,
    max_tokens: Optional[int] = None,
) -> ChatGroq:
    key = groq_api_key or os.getenv("GROQ_API_KEY")
    if not key:
        raise ValueError("Thiáº¿u GROQ_API_KEY")
    return ChatGroq(
        model=model,
        temperature=temperature,
        max_tokens=max_tokens,
    )


SYSTEM_INSTRUCTIONS = (
    "Báº¡n lÃ  trá»£ lÃ½ tráº£ lá»i vá» ChÆ°Æ¡ng TrÃ¬nh ÄÃ o Táº¡o UIT (KhÃ³a 2025).\n"
    "NguyÃªn táº¯c:\n"
    "â€¢ Chá»‰ dÃ¹ng thÃ´ng tin Ä‘Ã£ Ä‘Æ°á»£c cung cáº¥p trong dá»¯ liá»‡u tham chiáº¿u (náº¿u cÃ³). KhÃ´ng bá»‹a.\n"
    "â€¢ Náº¿u dá»¯ liá»‡u chÆ°a Ä‘á»§ Ä‘á»ƒ káº¿t luáº­n, nÃ³i ngáº¯n gá»n ráº±ng chÆ°a Ä‘á»§ vÃ  gá»£i Ã½ cÃ¡ch há»i cá»¥ thá»ƒ hÆ¡n.\n"
    "â€¢ Tráº£ lá»i tiáº¿ng Viá»‡t chuáº©n, ngáº¯n gá»n, máº¡ch láº¡c; cÃ³ thá»ƒ dÃ¹ng gáº¡ch Ä‘áº§u dÃ²ng khi phÃ¹ há»£p.\n"
    "â€¢ KhÃ´ng Ä‘á» cáº­p Ä‘áº¿n quy trÃ¬nh ná»™i bá»™, cÃ´ng cá»¥ hay cÃ¡ch báº¡n cÃ³ dá»¯ liá»‡u.\n"
)

ANSWER_PROMPT = ChatPromptTemplate.from_messages(
    [
        ("system", SYSTEM_INSTRUCTIONS),
        (
            "user",
            "CÃ¢u há»i: {question}\n\n"
            "Dá»¯ liá»‡u tham chiáº¿u (cÃ³ thá»ƒ trá»‘ng):\n"
            "----------------\n"
            "{contexts}\n"
            "----------------\n\n"
            "YÃªu cáº§u: Dá»±a vÃ o dá»¯ liá»‡u trÃªn Ä‘á»ƒ tráº£ lá»i ngáº¯n gá»n, rÃµ rÃ ng. "
            "Náº¿u dá»¯ liá»‡u khÃ´ng Ä‘á»§, hÃ£y nÃ³i ráº±ng chÆ°a Ä‘á»§ thÃ´ng tin vÃ  gá»£i Ã½ cÃ¡ch há»i cá»¥ thá»ƒ hÆ¡n."
        ),
    ]
)


def _collect_tool_chunks_from_state(state: MessagesState) -> List[Dict[str, Any]]:
    """Láº¥y artifact tá»« cÃ¡c ToolMessage gáº§n nháº¥t (náº¿u cÃ³)."""
    recent_tool_messages = []
    for message in reversed(state["messages"]):
        if message.type == "tool":
            recent_tool_messages.append(message)
        else:
            break
    tool_messages = recent_tool_messages[::-1]

    chunks: List[Dict[str, Any]] = []
    for m in tool_messages:
        if hasattr(m, "artifact") and isinstance(m.artifact, list):
            for c in m.artifact:
                if isinstance(c, dict):
                    content = (c.get("content") or "").strip()
                    if content:
                        chunks.append({"content": content})
    return chunks


def _format_context_plain(chunks: List[Dict[str, Any]], max_chars: int = 8000, max_items: int = 6) -> str:
    """
    Biáº¿n danh sÃ¡ch chunk thÃ nh má»™t khá»‘i vÄƒn báº£n pháº³ng, khÃ´ng chá»©a CTX id/source.
    """
    buf, total = [], 0
    for c in chunks[:max_items]:
        piece = (c.get("content") or "").strip()
        if not piece:
            continue
        if total + len(piece) > max_chars:
            break
        buf.append(piece)
        total += len(piece)
    return "\n\n".join(buf)


def _get_last_user_question(state: MessagesState) -> str:
    for m in reversed(state["messages"]):
        if m.type == "human":
            return str(m.content or "")
    return ""


def create_rag_chain(
    groq_api_key: Optional[str],
    *,
    model: str = "llama-3.3-70b-versatile",
    temperature: float = 0.2,
    k: int = 6,
    max_ctx_chars: int = 8000,
) -> Callable[[str], Dict[str, Any]]:

    _ensure_loaded()

    llm_decider = get_groq_llm(
        groq_api_key, model=model, temperature=temperature, max_tokens=7000
    ).bind_tools([retrieve])

    llm_answer = get_groq_llm(
        groq_api_key, model=model, temperature=temperature, max_tokens=7000
    )

    def query_or_response(state: MessagesState):
        """
        Node 1: Cho phÃ©p LLM quyáº¿t Ä‘á»‹nh cÃ³ cáº§n tool.
        """
        planner_system = SystemMessage(
            content=(
                "Báº¡n lÃ  má»™t trá»£ lÃ½ thÃ´ng minh cho CTÄT UIT. "
                "Náº¿u cÃ¢u há»i cáº§n chi tiáº¿t cá»¥ thá»ƒ tá»« dá»¯ liá»‡u chÆ°Æ¡ng trÃ¬nh (mÃ´n há»c, tÃ­n chá»‰, há»c ká»³, Ä‘iá»u kiá»‡n, quy Ä‘á»‹nh...), "
                "hÃ£y gá»i cÃ´ng cá»¥ 'retrieve' vá»›i truy váº¥n ngáº¯n gá»n tiáº¿ng Viá»‡t. "
                "Náº¿u cÃ³ thá»ƒ tráº£ lá»i ngay, Ä‘á»«ng gá»i cÃ´ng cá»¥."
            )
        )
        response = llm_decider.invoke([planner_system] + state["messages"])
        return {"messages": [response]}

    def generate_with_context(state: MessagesState):
        """
        Node 2: Sau khi (cÃ³ thá»ƒ) Ä‘Ã£ gá»i tool, tá»•ng há»£p tráº£ lá»i báº±ng PromptTemplate.
        KHÃ”NG nháº¯c Ä‘áº¿n trÃ­ch dáº«n/nguá»“n, KHÃ”NG hiá»ƒn thá»‹ source.
        """
        chunks = _collect_tool_chunks_from_state(state)
        contexts = _format_context_plain(chunks, max_chars=max_ctx_chars)
        question = _get_last_user_question(state)
        messages = ANSWER_PROMPT.format_messages(question=question, contexts=contexts)
        out = llm_answer.invoke(messages)
        return {"messages": [out]}

    tools = ToolNode([retrieve])

    graph_builder = StateGraph(MessagesState)
    graph_builder.add_node("query_or_response", query_or_response)
    graph_builder.add_node("tools", tools)
    graph_builder.add_node("generate_with_context", generate_with_context)

    graph_builder.add_edge(START, "query_or_response")
    graph_builder.add_conditional_edges("query_or_response", tools_condition, {END: END, "tools": "tools"})
    graph_builder.add_edge("tools", "generate_with_context")
    graph_builder.add_edge("generate_with_context", END)
    graph = graph_builder.compile()

    def qa(question: str, topk: Optional[int] = None) -> Dict[str, Any]:
        result = graph.invoke({"messages": [HumanMessage(content=question)]})
        try:
            print(f"\nâ“ CÃ¢u há»i: {question}")
            for m in result.get("messages", []):
                if getattr(m, "type", None) == "ai":
                    print("ğŸ¤– Tráº£ lá»i:", getattr(m, "content", ""))
                elif getattr(m, "type", None) == "tool":
                    print("ğŸ› ï¸ Tool output:", str(getattr(m, "content", "")))
        except Exception:
            pass
        return result

    return qa
